<div align="center">

<img src="https://anycrawl.dev/logo.svg" alt="AnyCrawl" height="100">
<h1>
  AnyCrawl
</h1>

<img src="https://img.shields.io/badge/‚ö°-Fast-blue" alt="Fast"/>
<img src="https://img.shields.io/badge/üöÄ-Scalable-orange" alt="Scalable"/>
<img src="https://img.shields.io/badge/üï∑Ô∏è-Web%20Crawling-ff69b4" alt="Web Crawling"/>
<img src="https://img.shields.io/badge/üåê-Site%20Crawling-9cf" alt="Site Crawling"/>
<img src="https://img.shields.io/badge/üîç-SERP%20(Multi%20Engines)-green" alt="SERP"/>
<img src="https://img.shields.io/badge/‚öôÔ∏è-Multi%20Threading-yellow" alt="Multi Threading"/>
<img src="https://img.shields.io/badge/üîÑ-Multi%20Process-purple" alt="Multi Process"/>
<img src="https://img.shields.io/badge/üì¶-Batch%20Tasks-red" alt="Batch Tasks"/>

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)
[![LLM Ready](https://img.shields.io/badge/LLM-Ready-blueviolet)](https://github.com/any4ai/anycrawl)
[![Documentation](https://img.shields.io/badge/üìñ-Documentation-blue)](https://docs.anycrawl.dev)

<p align="center">
  <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" alt="Node.js"/>
  <img src="https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript"/>
  <img src="https://img.shields.io/badge/Redis-DC3
  2D?style=for-the-badge&logo=redis&logoColor=white" alt="Redis"/>
</p>

</div>

## üìñ Overview

AnyCrawl is a high‚Äëperformance crawling and scraping toolkit:

- **SERP crawling**: multiple search engines, batch‚Äëfriendly
- **Web scraping**: single‚Äëpage content extraction
- **Site crawling**: full‚Äësite traversal and collection
- **High performance**: multi‚Äëthreading / multi‚Äëprocess
- **Batch tasks**: reliable and efficient
- **AI extraction**: LLM‚Äëpowered structured data (JSON) extraction from pages

LLM‚Äëfriendly. Easy to integrate and use.

## üöÄ Quick Start

üìñ See full docs: [Docs](https://docs.anycrawl.dev)

## üìö Usage Examples

üí° Use the [Playground](https://anycrawl.dev/playground) to test APIs and generate code in your preferred language.

> If self‚Äëhosting, replace `https://api.anycrawl.dev` with your own server URL.

### Web Scraping (Scrape)

#### Example

```typescript

curl -X POST https://api.anycrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_ANYCRAWL_API_KEY' \
  -d '{
  "url": "https://example.com",
  "engine": "cheerio"
}'

```


#### Parameters

| Parameter | Type              | Description                                                                                                                                                                       | Default  |
| --------- | ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| url       | string (required) | The URL to be scraped. Must be a valid URL starting with http:// or https://                                                                                                      | -        |
| engine    | string            | Scraping engine to use. Options: `cheerio` (static HTML parsing, fastest), `playwright` (JavaScript rendering with modern engine), `puppeteer` (JavaScript rendering with Chrome) | cheerio  |
| proxy     | string            | Proxy URL for the request. Supports HTTP and SOCKS proxies. Format: `http://[username]:[password]@proxy:port`                                                                     | _(none)_ |

More parameters: see [Request Parameters](https://docs.anycrawl.dev/en/general/scrape#request-parameters).


#### LLM Extraction

```bash
curl -X POST "https://api.anycrawl.dev/v1/scrape" \
  -H "Authorization: Bearer YOUR_ANYCRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "json_options": {
      "schema": {
        "type": "object",
        "properties": {
          "company_mission": { "type": "string" },
          "is_open_source": { "type": "boolean" },
          "employee_count": { "type": "number" }
        },
        "required": ["company_mission"]
      }
    }
  }'
```

### Site Crawling (Crawl)

#### Example

```typescript

curl -X POST https://api.anycrawl.dev/v1/crawl \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_ANYCRAWL_API_KEY' \
  -d '{
  "url": "https://example.com",
  "engine": "playwright",
  "max_depth": 2,
  "limit": 10,
  "strategy": "same-domain"
}'

```

#### Parameters

| Parameter     | Type                | Description                                                                                 | Default       |
| ------------- | ------------------- | ------------------------------------------------------------------------------------------- | ------------- |
| url           | string (required)   | Starting URL to crawl                                                                       | -             |
| engine        | string              | Crawling engine. Options: `cheerio`, `playwright`, `puppeteer`                              | cheerio       |
| max_depth     | number              | Max depth from the start URL                                                                | 10            |
| limit         | number              | Max number of pages to crawl                                                                | 100           |
| strategy      | enum                | Scope: `all`, `same-domain`, `same-hostname`, `same-origin`                                 | same-domain   |
| include_paths | array<string>       | Only crawl paths matching these patterns                                                    | _(none)_      |
| exclude_paths | array<string>       | Skip paths matching these patterns                                                          | _(none)_      |
| scrape_options| object              | Per-page scrape options (formats, timeout, json extraction, etc.), same as Scrape options   | _(none)_ |

More parameters and endpoints: see [Request Parameters](https://docs.anycrawl.dev/en/general/scrape#request-parameters).


### Search Engine Results (SERP)

#### Example

```typescript
curl -X POST https://api.anycrawl.dev/v1/search \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_ANYCRAWL_API_KEY' \
  -d '{
  "query": "AnyCrawl",
  "limit": 10,
  "engine": "google",
  "lang": "all"
}'
```

#### Parameters

| Parameter | Type              | Description                                                | Default |
| --------- | ----------------- | ---------------------------------------------------------- | ------- |
| `query`   | string (required) | Search query to be executed                                | -       |
| `engine`  | string            | Search engine to use. Options: `google`                    | google  |
| `pages`   | integer           | Number of search result pages to retrieve                  | 1       |
| `lang`    | string            | Language code for search results (e.g., 'en', 'zh', 'all') | en-US   |

#### Supported search engines

- Google

## ‚ùì FAQ

1. **Can I use proxies?** Yes. AnyCrawl ships with a high‚Äëquality default proxy. You can also configure your own: set the `proxy` request parameter (per request) or `ANYCRAWL_PROXY_URL` (self‚Äëhosting).
2. **How to handle JavaScript‚Äërendered pages?** Use the `Playwright` or `Puppeteer` engines.

## ü§ù Contributing

We welcome contributions! See the [Contributing Guide](CONTRIBUTING.md).

## üìÑ License

MIT License ‚Äî see [LICENSE](LICENSE).

## üéØ Mission

We build simple, reliable, and scalable tools for the AI ecosystem.

---

<div align="center">
  <sub>Built with ‚ù§Ô∏è by the Any4AI team</sub>
</div>
